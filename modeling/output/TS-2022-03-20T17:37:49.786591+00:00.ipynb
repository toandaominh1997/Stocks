{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02bb6a28",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [2]</a>'.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbae6d91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T17:37:59.937476Z",
     "iopub.status.busy": "2022-03-20T17:37:59.937106Z",
     "iopub.status.idle": "2022-03-20T17:37:59.963873Z",
     "shell.execute_reply": "2022-03-20T17:37:59.962720Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.075687,
     "end_time": "2022-03-20T17:37:59.969658",
     "exception": false,
     "start_time": "2022-03-20T17:37:59.893971",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "msgs = \"Ran from Airflow at 2022-03-20T17:37:49.786591+00:00!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7883108c",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faa09b38-cc7a-4699-b1e2-0e59ae4a0e7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-20T17:38:00.071620Z",
     "iopub.status.busy": "2022-03-20T17:38:00.071320Z",
     "iopub.status.idle": "2022-03-20T17:38:05.167726Z",
     "shell.execute_reply": "2022-03-20T17:38:05.166570Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 5.141344,
     "end_time": "2022-03-20T17:38:05.171125",
     "exception": true,
     "start_time": "2022-03-20T17:38:00.029781",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'category_encoders'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmultioutput\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiOutputRegressor\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Category Encoder\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcategory_encoders\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mce\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# pyod\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyod\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabod\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ABOD\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'category_encoders'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "# pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "from sklearn import set_config\n",
    "set_config(display = 'diagram')\n",
    "from pandas.api.types import infer_dtype\n",
    "\n",
    "\n",
    "# Scikit Learn import\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "# from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split, TimeSeriesSplit\n",
    "from sklearn.preprocessing import RobustScaler, LabelEncoder\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn import cluster\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler, QuantileTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "\n",
    "# Category Encoder\n",
    "import category_encoders as ce\n",
    "# pyod\n",
    "from pyod.models.abod import ABOD\n",
    "from pyod.models.cblof import CBLOF\n",
    "from pyod.models.cof import COF\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.hbos import HBOS\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "from pyod.models.pca import PCA as PCA_pyod\n",
    "from pyod.models.mcd import MCD\n",
    "from pyod.models.sod import SOD\n",
    "from pyod.models.sos import SOS\n",
    "# model\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import re\n",
    "\n",
    "import vnquant.DataLoader as dl\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9690df38-50ea-48b7-bb2b-1c7b0ef81cb0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "init_date = '2020-01-01'\n",
    "start_date = '2021-01-01'\n",
    "end_date = '2022-03-20'\n",
    "symbols = [\"HSG\", \"HPG\", \"NKG\"]\n",
    "def get_data(symbol, start_date = start_date, end_date = end_date):\n",
    "    loader = dl.DataLoader(symbols=[symbol], start=start_date, end=end_date, minimal=True)\n",
    "    data = loader.download()\n",
    "    data.columns = [col[0] for col in data.columns]\n",
    "    data = data.reset_index()\n",
    "    data['symbol'] = symbol\n",
    "    data['target_day_1'] = data.close.shift(-1)\n",
    "    data['target_day_2'] = data.close.shift(-2)\n",
    "    data['target_day_3'] = data.close.shift(-3)\n",
    "    data['target_day_4'] = data.close.shift(-4)\n",
    "    data['target_day_5'] = data.close.shift(-5)\n",
    "    return data\n",
    "df = pd.concat([get_data(symbol) for symbol in symbols], axis = 0)\n",
    "df = df.sort_values(by=['date'])\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4365e491-c7b4-4e41-8472-c75af052e4db",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "numeric_columns = [\"high\", \"low\", \"open\", \"close\", \"volume\"]\n",
    "category_columns = ['symbol']\n",
    "time_columns = ['date']\n",
    "feature_columns = numeric_columns+category_columns+time_columns\n",
    "target_columns = ['target_day_1', 'target_day_2', 'target_day_3', 'target_day_4', 'target_day_5']\n",
    "# X = df.loc[df[target_columns].isnull().any(axis = 1)].drop(columns=target_columns)\n",
    "# y = df.loc[df[target_columns].notnull().any(axis = 1)][target_columns]\n",
    "# test = df.loc[df[target_columns].isnull().any(axis = 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91d42b4-d7c1-4a13-81bf-ac1ddd26d99b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TimePreprocess(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, list_of_features=[\"day\", \"dayofweek\", \"month\"],):\n",
    "        super().__init__()\n",
    "        self.time_features = []\n",
    "        self.list_of_features = list_of_features\n",
    "    def fit(self, X, y = None):\n",
    "        for col in X.columns:\n",
    "            try:\n",
    "                X[col] = pd.to_datetime(X[col])\n",
    "                self.time_features.append(col)\n",
    "            except:\n",
    "                logger.error('Time columns cannot be convert to datetime')\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        columns = []\n",
    "        for col in self.time_features:\n",
    "            X[col] = pd.to_datetime(X[col])\n",
    "            if 'day' in self.list_of_features:\n",
    "                columns.append(col+'_day')\n",
    "                X[columns[-1]] = X[col].dt.day\n",
    "            if 'month' in self.list_of_features:\n",
    "                columns.append(col+'_month')\n",
    "                X[columns[-1]] = X[col].dt.month\n",
    "            if 'dayofweek' in self.list_of_features:\n",
    "                columns.append(col+'_dayofweek')\n",
    "                X[columns[-1]] = X[col].dt.dayofweek\n",
    "        return X[columns]\n",
    "    def fit_transform(self, X, y = None):\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X)\n",
    "\n",
    "\n",
    "numeric_preprocess  = Pipeline(steps = [\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "    ('scaler_quantile', QuantileTransformer()),\n",
    "    # ('scaler', MinMaxScaler())\n",
    "])\n",
    "category_preprocess = Pipeline(steps = [\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', ce.OneHotEncoder(use_cat_names = True)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b096c03-2e1c-494b-83d3-4671e543447c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "preprocessing = ColumnTransformer(transformers=[\n",
    "        ('numeric_preprocess', numeric_preprocess, numeric_columns),\n",
    "        ('category_columns', category_preprocess, category_columns),\n",
    "        ('time_preprocess', TimePreprocess(), time_columns),\n",
    "        # ('hold_feature', HoldFeature(), hold_columns),\n",
    "    ],\n",
    "                                      n_jobs = -1\n",
    "                                     )\n",
    "\n",
    "pipe = Pipeline(steps = [\n",
    "    ('preprocessing', preprocessing),\n",
    "    ('estimator', CatBoostRegressor())\n",
    "])\n",
    "# pipe.fit(X, y)\n",
    "params = {\n",
    "    'estimator': [CatBoostRegressor(verbose = 0),\n",
    "                  # LGBMRegressor(verbose = 0),\n",
    "                  # XGBRegressor()\n",
    "                 ]\n",
    "}\n",
    "grid =  GridSearchCV(estimator = pipe, \n",
    "                     param_grid = params,\n",
    "                     scoring = 'neg_mean_squared_error',\n",
    "                     cv = TimeSeriesSplit(n_splits=5),\n",
    "                     n_jobs = -1\n",
    "                    )\n",
    "for target_col in target_columns:\n",
    "    X = df.loc[df[target_col].notnull(), feature_columns]\n",
    "    y = df.loc[df[target_col].notnull(), target_col]\n",
    "    grid.fit(X, y)\n",
    "    print(grid.cv_results_)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fd95fd-61f3-494e-b2bb-fd4c8ebe7dc6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "grid.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efae954-d3de-479f-af51-1f0a238fa2c6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test['target'] = grid.best_estimator_.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a26e97-1f2f-4e33-ba7b-61bfa7aeb998",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb1acd7-22b7-47f1-8928-a37c398bce93",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c40b2a2-1973-465d-ad4f-4a4a1e81e9f1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12.538129,
   "end_time": "2022-03-20T17:38:05.813717",
   "environment_variables": {},
   "exception": true,
   "input_path": "/opt/code/modeling/TS.ipynb",
   "output_path": "/opt/code/modeling/output/TS-2022-03-20T17:37:49.786591+00:00.ipynb",
   "parameters": {
    "msgs": "Ran from Airflow at 2022-03-20T17:37:49.786591+00:00!"
   },
   "start_time": "2022-03-20T17:37:53.275588",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}